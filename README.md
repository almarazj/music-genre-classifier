# ğŸ† Machine Learning Challenge: Audio Specialization
### Tabla de contenidos
- [ğŸŒ± IntroducciÃ³n](#ğŸŒ±-introducciÃ³n)
- [â„¹ï¸ Consideraciones generales del desarrollo](#â„¹ï¸-consideraciones-generales-del-desarrollo)
- [ğŸ“¨ Entrega del challenge](#ğŸ“¨-entrega-del-challenge)
- [ğŸ§© Preprocesamiento](#ğŸ§©-preprocesamiento)
- [ğŸ¤– DiseÃ±o del modelo](#ğŸ¤–-diseÃ±o-del-modelo)
- [ğŸ‹ï¸â€â™‚ï¸ Entrenamiento](#ğŸ‹ï¸â€â™‚ï¸-entrenamiento)
- [ğŸª„ Inferencia](#ğŸª„-inferencia)
- [ğŸ“š BibliografÃ­a](#ğŸ“š-bibliografÃ­a)

---
# ğŸŒ± IntroducciÃ³n
El siguiente challenge tiene como finalidad implementar un clasificador simple de gÃ©neros musicales para introducir la lÃ³gica, arquitectura, estructura y metodologÃ­as implementadas en el desarrollo de proyectos de Machine Learning.

Se evaluarÃ¡ el desarrollo en las siguientes categorÃ­as: 
- Prolijidad y legibilidad del cÃ³digo.
- Criterio en el diseÃ±o de funciones y/o clases.
- Lectura e implementaciÃ³n criteriosa de la documentaciÃ³n.
- ComunicaciÃ³n con el resto del equipo ante dudas y/o inconvenientes.
- Uso mÃ­nimo de versionado de cÃ³digo.

---

# â„¹ï¸ Consideraciones generales del desarrollo

- Cada persona que lleve a cabo el challenge, debe **desarrollar todo su cÃ³digo en una nueva rama del repositorio**, la cual se debe llamar `challenge/usuario-de-github` con su usuario de GitHub pertinente.

- El cÃ³digo desarrollado debe estar en idioma **ingles**, lo cual abarca:
    - Nombres de variables
    - Nombres de funciones/clases
    - Comentarios en el documento
    - DocumentaciÃ³n (docstrings)

- Cada funciÃ³n, clase o mÃ©todo debe ser correctamente documentado a travÃ©s de sus **docstrings** en **formato numpy**.
- Para poder replicar el entorno de desarrollo, se debe **generar un archivo** `requirements.txt` con las librerÃ­as utilizadas a lo largo del challenge, como sus versiones.

---

# ğŸ“¨ Entrega del challenge 

- La entrega final de challenge serÃ¡ mediante un **pull request** hacia la rama `main` del repositorio. En el mismo deberÃ¡n incluir **capturas de pantalla** con el log de las ultimas epochs del entrenamiento, exhibiendo las mÃ©tricas, asi como los prints en consola con la inferencia.

---
# ğŸ§© Preprocesamiento
El preprocesamiento del dataset se debe realizar dentro del script `challenge/preprocessing.py` con los siguientes requerimientos:
### **Requerimientos**
- En el preprocesamiento se debe poder splitear el audio en `n` segmentos, de forma que aumente la cantidad de muestras en el dataset.
- Para no procesar los audios como series temporales, se deben transformar los audios en **espectrogramas de Mel** (*Consejo: Utilizar funcionalidades de la librerÃ­a Librosa*)
- El dataset deberÃ¡ ser procesado por gÃ©nero, y **cada espectrograma debe ser guardado en un** `.npz` (NumPy Compressed file), es decir, un archivo `.npz` por espectrograma. Investigar que funciones de NumPy permiten guardar un array de forma comprimida.
    - Para que el Dataset y DataLoader funcionen correctamente (los cuales estÃ¡n dentro de `challenge/data_managment/dataset.py` y ya estÃ¡n codeados), el nombre de la variable dentro de la funciÃ³n de guardado se debe llamar `spectrogram`
- Los archivos deben estÃ¡r organizados por gÃ©nero dentro de la carpeta  `genres_mel_npz/`, la cual se aloja dentro de `dataset/`.
- **La estructura de carpetas y archivos debe quedar asÃ­**:
    ```python
    dataset/
        â”œâ”€â”€ genres_original/
        â”œâ”€â”€ images_original/
        â”œâ”€â”€ features_3_sec.csv
        â”œâ”€â”€ features_30_sec.csv
        â””â”€â”€ genres_mel_npz/
                â”œâ”€â”€ blues
                |    â”œâ”€â”€ blues.00000-0.npz
                |    â”œâ”€â”€ blues.00000-1.npz
                |    â”œâ”€â”€ blues.00000-2.npz
                |    â”œâ”€â”€ blues.00000-3.npz
                |    â”œâ”€â”€ blues.00000-4.npz
                |    â””â”€â”€ ...
                â”œâ”€â”€ rock
                â””â”€â”€ ...
    ```

### **RecomendaciÃ³n**
- Recordar que las transformaciones aplicadas al dataset original se deben poder aplicar tambien en la etapa de inferencia del modelo, es decir, si mi modelo estÃ¡ entrenado con espectrogramas de Mel, la inferencia la debo hacer con espectrogramas de Mel tambien. Por lo tanto, si quiero inferir un audio representado por una serie temporal (archivo .wav, .mp3, etc), se le debe aplicar el mismo preprocesamiento que se le aplicÃ³ al conjunto de entrenamiento.

---

# ğŸ¤– DiseÃ±o del modelo
Se solicita diseÃ±ar una **red neuronal convolucional** con 3 capaz convolutivas, seguido de una capa "aplanadora" (flatten) y un clasificador. La misma debe ser desarrollada dentro del script `challenge/models/cnn.py`

### **Requerimientos**

- El modelo debe ser construido utilizando **mÃ©todos de PyTorch**. La arquitectura de la misma debe ser replicada del siguiente fragmento de cÃ³digo, donde se define la arquitectura utilizando mÃ©todos de **Keras**

- El objetivo principal es **practicar y agilizar la lectura de documentaciÃ³n**, tanto de TensorFlow como de PyTorch, para analizar las similitudes y diferencias entre estas dos librerias.

    ```python
    # 1st conv layer
    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(...)))
    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
    model.add(keras.layers.BatchNormalization())

    # 2nd conv layer
    model.add(keras.layers.Conv2D(32, (3, 3), activation='relu'))
    model.add(keras.layers.MaxPooling2D((3, 3), strides=(2, 2), padding='same'))
    model.add(keras.layers.BatchNormalization())

    # 3rd conv layer
    model.add(keras.layers.Conv2D(32, (2, 2), activation='relu'))
    model.add(keras.layers.MaxPooling2D((2, 2), strides=(2, 2), padding='same'))
    model.add(keras.layers.BatchNormalization())

    # flatten output and feed it into dense layer
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dense(64, activation='relu'))
    model.add(keras.layers.Dropout(0.3))

    # output layer
    model.add(keras.layers.Dense(10, activation='softmax'))
    ```

---

# ğŸ‹ï¸â€â™‚ï¸ Entrenamiento
El entrenamiento debe ser desarrollado dentro del script `challenge/training.py`.

### **Requerimientos**

- Las **mÃ©tricas** para evaluar el rendimiento del modelo serÃ¡n la pÃ©rdida (`loss`) y la presiciÃ³n (`accuracy`).

- Dentro de este script, se debe definir el loop de entrenamiento, el loop de validaciÃ³n, el cÃ¡lculo de mÃ©tricas por batch, y el **guardado del modelo entrenado**.

- A su vez, para calcular la pÃ©rdida por batch se debe utilizar la entropÃ­a cruzada (`CrossEntropyLoss`), y la optimizaciÃ³n debe ser implementada con el algoritmo de `Adam`. 

- Con respecto a los **hiperparÃ¡metros del modelo**, para poder comparar resultados, se precisa:

    - Epochs = 300
    - Learning rate = 1e-3


---

# ğŸª„ Inferencia

La inferencia se debe realizar dentro del script `challenge/inference.py`

### **Requerimientos**

- En primer lugar, se debe cargar el **modelo guardado** en la carpeta `results/`
- A la hora de exhibir los resultados, se debe imprimir en consola el gÃ©nero predicho.

---

# ğŸ“š BibliografÃ­a
[Understanding the Mel Spectrogram - Medium](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53)

[Feature extraction - Librosa](https://librosa.org/doc/main/feature.html)

[Training a classifier - PyTorch Tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)

[Learning PyTorch for Deep Learning in a day. Literally - Youtube](https://www.youtube.com/watch?v=Z_ikDlimN6A&t=37814s&ab_channel=DanielBourke)


